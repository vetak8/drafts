{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1738f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import (AdaBoostClassifier, GradientBoostingClassifier,\n",
    "                              RandomForestClassifier, ExtraTreesClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats.distributions import randint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0e931",
   "metadata": {},
   "source": [
    "Разберём стекинг на практике, проанализировав датасет, описывающий параметры, которые были сняты со спутника при фотографировании Земли. У нас есть 54 переменные. Для упрощения будем рассматривать два типа поверхностей —так мы сводим нашу задачу к задаче бинарной классификации. Сделаем базовую предобработку, воспользуемся StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e7c0d",
   "metadata": {},
   "source": [
    "Стекинг позволяет объединять ответы нескольких алгоритмов первого уровня в один большой ответ при помощи нового алгоритма обучения. Для избежания переобучения будем разбивать обучающую выбоку на фолды — разбиение обучающей выборки на несколько частей. На каждом фолде мы обучаем алгоритм заново.\n",
    "\n",
    "Функция compute_meta_feature принимает на вход один алгоритм и возвращает новые признаки на объектах, которые не использовались во время обучения. Функция generate_meta_feature делает подобное, но принимает на вход несколько классификаторов в списке и повторяет процедуру, а затем генерирует и возвращает матрицу с изначальным количеством объектов, признаков будет столько, сколько мы передали классификаторов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241eeb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meta_feature(clf, X_train, X_test, y_train, cv):\n",
    "    \n",
    "    X_meta_train = np.zeros_like(y_train, dtype=np.float32)\n",
    "    for train_fold_index, predict_fold_index in cv.split(X_train):\n",
    "        X_fold_train, X_fold_predict = X_train[train_fold_index], X_train[predict_fold_index]\n",
    "        y_fold_train = y_train[train_fold_index]\n",
    "        \n",
    "        folded_clf = clone(clf)\n",
    "        folded_clf.fit(X_fold_train, y_fold_train)\n",
    "        X_meta_train[predict_fold_index] = folded_clf.predict_proba(X_fold_predict)[:, 1]\n",
    "    \n",
    "    meta_clf = clone(clf)\n",
    "    meta_clf.fit(X_train, y_train)\n",
    "    \n",
    "    X_meta_test = meta_clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return X_meta_train, X_meta_test\n",
    "\n",
    "def generate_meta_features(classifiers, X_train, X_test, y_train, cv):\n",
    "   \n",
    "    features = [\n",
    "        compute_meta_feature(clf, X_train, X_test, y_train, cv)\n",
    "        for clf in tqdm(classifiers)\n",
    "    ]\n",
    "    \n",
    "    stacked_features_train = np.vstack([\n",
    "        features_train for features_train, features_test in features\n",
    "    ]).T\n",
    "\n",
    "    stacked_features_test = np.vstack([\n",
    "        features_test for features_train, features_test in features\n",
    "    ]).T\n",
    "    \n",
    "    return stacked_features_train, stacked_features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5fad6",
   "metadata": {},
   "source": [
    "Обучим один градиентный бустинг, в котором будем использовать 300 алгоритмов. Мы получим accuracy = 0.7899929527836504. Если мы будем агрегировать бустинг со случайным лесом и с двумя логистическими регрессиями, то получим результат лучше: 0.8118393234672304.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dff1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_digits()\n",
    "X, y = dataset['data'], dataset['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350af7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db003e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meta_feature(clf, X_train, X_test, y_train, cv):\n",
    "    \"\"\"    Эта функция подсчитывает признаки для мета-классификатора.    \n",
    "    Они являются вероятностями классов при решении задачи многоклассовой классификации. \n",
    "    :arg clf: классификатор    :args X_train, y_train: обучающая выборка   \n",
    "    :arg X_test: признаки тестовой выборки    :arg cv: класс, генерирующий фолды (KFold)    \n",
    "    :returns X_meta_train, X_meta_test: новые признаки для обучающей и тестовой выборок    \"\"\"\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    X_meta_train = np.zeros((len(X_train), n_classes), dtype=np.float32)\n",
    "    for train_fold_index, predict_fold_index in cv.split(X_train):\n",
    "        X_fold_train, X_fold_predict = X_train[train_fold_index], X_train[predict_fold_index]\n",
    "        y_fold_train = y_train[train_fold_index]\n",
    "\n",
    "        folded_clf = clone(clf)\n",
    "        folded_clf.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        X_meta_train[predict_fold_index] = folded_clf.predict_proba(\n",
    "            X_fold_predict)\n",
    "\n",
    "    meta_clf = clone(clf)\n",
    "    meta_clf.fit(X_train, y_train)\n",
    "\n",
    "    X_meta_test = meta_clf.predict_proba(X_test)\n",
    "\n",
    "    return X_meta_train, X_meta_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ec544",
   "metadata": {},
   "source": [
    "Задание 6.6.1\n",
    "1 point possible (graded)\n",
    "В скринкасте мы разобрали схему генерации признаков в стекинге, когда для тестовой выборки алгоритм заново переобучался на всей тренировочной выборке. Реализуйте схему, когда вместо этого производится агрегация ответов всех обученных на фолдах классификаторов на тестовой выборке при помощи усреднения.\n",
    "\n",
    "Логика решения:\n",
    "1) Создадим X_meta_test, заполним его нулями (по аналогии с X_meta_train);\n",
    "2) Далее на каждом шаге, где мы обучаем folded_clf.fit (X_fold_train, y_fold_train) и его предсказания на X_fold_predict запихиваем в X_meta_train[predict_fold_index] добавим еще одну строку, где в X_meta_test будем добавлять предсказания вероятностей folded_clf на X_test. Их можно сразу складывать друг с другом или сохранить много массивов, тогда в конце их нужно будет все сложить, а потом делить на количество сплитов (количество массивов равно количеству сплитов в кросс - валидации);\n",
    "3) После цикла останется только усреднить все эти массивы, это и будет наш X_meta_test.\n",
    "За основу нужно взять следующий код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8efedb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meta_feature_mean(clf, X_train, X_test, y_train, cv):\n",
    "    \"\"\"\n",
    "    Эта функция подсчитывает признаки для мета-классификатора. \n",
    "    Они являются вероятностями классов при решении задачи многоклассовой классификации.\n",
    "\n",
    "    :arg clf: классификатор\n",
    "    :args X_train, y_train: обучающая выборка\n",
    "    :arg X_test: признаки тестовой выборки\n",
    "    :arg cv: класс, генерирующий фолды (KFold)\n",
    "\n",
    "    :returns X_meta_train, X_meta_test: новые признаки для обучающей и тестовой выборок\n",
    "    \"\"\"\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    X_meta_train = np.zeros(len(y_train),n_classes, dtype=float32)\n",
    "    for  train_fold_index, predict_fold_index in cv.split(X_train):\n",
    "        X_fold_train = X_train[train_fold_index]\n",
    "        X_fold_predict = X_train[predict_fold_index]\n",
    "        y_fold_train = y_train[train_fold_index]       \n",
    "        \n",
    "        folded_clf = clone(clf)\n",
    "        folded_clf.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        X_meta_train[predict_fold_index] = folded_clf.predict_proba(X_fold_predict)\n",
    "        X_meta_test += folded_clf.predict_proba(X_test)\n",
    "        \n",
    "    \n",
    "    \n",
    "    X_meta_test = X_meta_test/cv.get_n_splits()\n",
    "    return X_meta_train, X_meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6acfdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d02793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meta_feature(clf, X_train, X_test, y_train, cv):\n",
    "    \n",
    "    n_classes = len(np.unique(y_train))\n",
    "    X_meta_train = np.zeros((len(y_train), n_classes), dtype=np.float32)\n",
    "\n",
    "    splits = cv.split(X_train)\n",
    "    for train_fold_index, predict_fold_index in splits:\n",
    "        X_fold_train, X_fold_predict = X_train[train_fold_index], X_train[predict_fold_index]\n",
    "        y_fold_train = y_train[train_fold_index]\n",
    "        \n",
    "        folded_clf = clone(clf)\n",
    "        folded_clf.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        X_meta_train[predict_fold_index] = folded_clf.predict_proba(X_fold_predict)\n",
    "    \n",
    "    meta_clf = clone(clf)\n",
    "    meta_clf.fit(X_train, y_train)\n",
    "    \n",
    "    X_meta_test = meta_clf.predict_proba(X_test)\n",
    "    \n",
    "    return X_meta_train, X_meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66dff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_meta_features(classifiers, X_train, X_test, y_train, cv):\n",
    "   \n",
    "    features = [\n",
    "        compute_meta_feature(clf, X_train, X_test, y_train, cv)\n",
    "        for clf in tqdm(classifiers)\n",
    "    ]\n",
    "    \n",
    "    stacked_features_train = np.hstack([\n",
    "        features_train for features_train, features_test in features\n",
    "    ])\n",
    "\n",
    "    stacked_features_test = np.hstack([\n",
    "        features_test for features_train, features_test in features\n",
    "    ])\n",
    "    \n",
    "    return stacked_features_train, stacked_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b757a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd10920a",
   "metadata": {},
   "source": [
    "Задание 6.6.2\n",
    "5 points possible (graded)\n",
    "Используйте функцию generate_meta_features для стекинга следующих алгоритмов:\n",
    "логистическая регрессия с L1-регуляризацией, C=0.001, солвер — 'saga', схема работы мультиклассовой классификации — one-vs-rest, максимальное допустимое количество итераций — 2000\n",
    "логистическая регрессия с L2-регуляризацией, C=0.001, солвер — 'saga', схема работы мультиклассовой классификации — multinomial, максимальное допустимое количество итераций — 2000\n",
    "случайный лес из 300 деревьев\n",
    "градиентный бустинг из 200 деревьев\n",
    "Как мета-алгоритм используйте логистическую регрессию без регуляризации со схемой работы мультиклассовой классификации — auto и солвером 'lbfgs'.\n",
    "Посчитайте качество при помощи передачи новых признаков в функцию compute_metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ebcac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "def compute_metric(clf, X_train=X_train, y_train=y_train, X_test=X_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    return np.round(f1_score(y_test, y_test_pred, average='macro'), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a70f1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [03:19<00:00, 50.00s/it]\n"
     ]
    }
   ],
   "source": [
    "stacked_features_train, stacked_features_test = generate_meta_features([\n",
    "    LogisticRegression(C=0.001, penalty='l1', solver='saga',\n",
    "                       max_iter=2000, multi_class='ovr', random_state=42),\n",
    "    LogisticRegression(C=0.001, penalty='l2', solver='saga',\n",
    "                       max_iter=2000, multi_class='ovr', random_state=42),\n",
    "    RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=42),\n",
    "    GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "], X_train, X_test, y_train, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82db22bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='none', random_state=42)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='none',multi_class='auto' ,solver='lbfgs',random_state=42)\n",
    "clf.fit(stacked_features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc4680bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.976219"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metric(clf,stacked_features_train,y_train,stacked_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ab784",
   "metadata": {},
   "source": [
    "Используйте функцию generate_meta_features для стекинга следующих алгоритмов: случайный лес из 300 деревьев случайный лес из 200 экстремальных деревьев Как мета-алгоритм используйте логистическую регрессию без регуляризации со схемой работы мультиклассовой классификации — auto и солвером 'lbfgs'. Посчитайте качество при помощи передачи новых признаков в функцию compute_metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2df237a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:17<00:00,  8.89s/it]\n"
     ]
    }
   ],
   "source": [
    "stacked_features_train, stacked_features_test = generate_meta_features([\n",
    "    RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=42),\n",
    "    RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n",
    "], X_train, X_test, y_train, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac2c55d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='none', random_state=42)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = LogisticRegression(penalty='none', solver='lbfgs',random_state=42)\n",
    "clf.fit(stacked_features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6c271c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.976633"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metric(clf,stacked_features_train,y_train,stacked_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa7a4f",
   "metadata": {},
   "source": [
    "Используйте функцию generate_meta_features для стекинга следующих алгоритмов:\n",
    "метод ближайшего соседа (k-NN) со стандартными параметрами\n",
    "случайный лес из 300 экстремальных деревьев\n",
    "Как мета-алгоритм используйте логистическую регрессию без регуляризации со схемой работы мультиклассовой классификации — auto и солвером 'lbfgs'.\n",
    "Посчитайте качество при помощи передачи новых признаков в функцию compute_metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8cb78157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "969d8467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.61s/it]\n"
     ]
    }
   ],
   "source": [
    "stacked_features_train, stacked_features_test = generate_meta_features([\n",
    "    KNeighborsClassifier(),\n",
    "    RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=42)\n",
    "], X_train, X_test, y_train, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "defeee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vetak\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='none', random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='none', solver='lbfgs',multi_class='auto',random_state=42)\n",
    "clf.fit(stacked_features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbe4579b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vetak\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.985552"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metric(clf,stacked_features_train,y_train,stacked_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4812fb89",
   "metadata": {},
   "source": [
    "Используйте функцию generate_meta_features для стекинга следующих алгоритмов: логистическая регрессия с L1-регуляризацией, C=0.001, солвер — 'saga', схема работы мультиклассовой классификации — one-vs-rest, максимальное допустимоей количество итераций — 2000 метод ближайшего соседа со стандартными параметрами случайный лес из 300 экстремальных деревьев AdaBoost со стандартными параметрами Как мета-алгоритм используйте логистическую регрессию без регуляризации со схемой работы мультиклассовой классификации — auto и солвером 'lbfgs'. Посчитайте качество при помощи передачи новых признаков в функцию compute_metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907253d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b294967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
